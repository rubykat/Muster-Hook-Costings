# Design

I want this CMS/Wiki to have the best bits from Contenticious, IkiWiki, and PmWiki.

* Mojolicious framework - done. Will need to be reworked a lot along the way, I expect.

## Multiple Page Directories

Similar to PageStores from PmWiki, but not having an abstract PageStore class; it's too complicated right now
to implement virtual pages, especially when I could do it as a different mojolicious app. So they're just page
directories, which contain files.

As with PmWiki, the page sources are ordered, and whichever page is found first is the one that takes precedence.

## First-Class Meta-Data

Have page fields built in - as with PmWiki's PageVariables.
For text-based pages, use a YAML prefix section.
For other kinds of pages, do things like get EXIF data.

## Transparent Parsing of Files of Different Types

From IkiWiki. There, one adds a plugin which parses a new filetype.
I have a partial implementation: new filetypes are done by adding a new module Muster::Leaf::File::*file-extension*.
If there isn't one, then the default Muster::Leaf::File will deal with non-pages (which have an empty pagetype).

## Separate Scanning Pass

This from IkiWiki; the scanning part of IkiWiki doesn't take long, it's the building of everything that does -- and that's my fault, because my plugins basically stuff up the dependency tree.
But I like the idea of having a scanning pass which collects up all the meta-data needed for later on.

I'm storing it in an SQLite database rather than what IkiWiki was using, because I like the flexibility of an SQL database for looking things up. While IkiWiki has a clever idea of page-conditions, why reinvent the wheel when SQL is such a powerful query language?

The types of data that need to be stored:

* known properties of pages (and files), such as pagename, title, filename etc
* multi-valued known properties of pages
    * links
    * siblings
    * children
* unknown arbitrary properties of pages that are needed by plugins. Done initially with a multi-valued deep table (page, field, value).
    * should only add to this table if the item is a page and not a non-page
* a flattened version of the deep table, so that all the properties of a page are in one row; this makes reporting easier
    * at the end of the scanning pass, (re)make this table, whose columns are all the fields from the deep table, and whose rows are all the values.

Tables:

* pagefiles: (page, title, parent_page, pagetype, filename...)
* links: (page, links_to)
* siblings: (page, sibling)
* children: (page, child)
* deepfields: (page, field, value)
* flatfields: (page, field1, field2, field3....)

Non-pages will be special "pages" with an empty type, and the URL will be parent/basename.ext (rather than parent/basename) and the binary file will be served.

Modes of scanning:

* scan everything
* scan one page
* delete one page

My intent is to eventually have all pages stored in a git repo, and have a git post-update hook which calls the scan-one-page for that file.
But that wouldn't cover the case of pages being deleted; not sure what to do about that one. I'd have to check how to do that with git hooks.
If the scan-everything pass is fast enough, it might be okay to call it on every post-update.

* will need to add a command to install appropriate git-hooks instead of doing it by hand
* may have to set up separate bare/web_src repos the way IkiWiki does it
* whether one is using git or not, need to have a way of triggering a scan when a change is made (as distinct from scanning every time a page is rendered, that's wasteful)

## Process Phases With Hooks

From both PmWiki and IkiWiki.
IkiWiki divides the process into distinct phases,
while PmWiki enables more fine-grained control of the order of when things are done, by allowing one to specify which "item" one's hook is inserted before or after.
See [Writing Plugins](http://ikiwiki.info/plugins/write/) for the full set of IkiWiki hooks.

A subset of relevant IkiWiki hooks:

* scan - grabs the info needed for all the later passes
* filter - runs on full raw source of the page
* preprocess - does the preprocess directives
* linkify - does the wiki links -- this needs the scan info to see which pages exist and which don't
* htmlize - convert to HTML
* sanitize - sanitize the HTML
* format - runs on the HTML version of the page

Whether I adopt those IkiWiki categories or not, there is certainly going to be a division between scanning and rendering.

## Plugins

From PmWiki and IkiWiki and other wikis.
I'm not sure what approach to take for this.
Already have "plugin" for file types, with Muster::Leaf::File::*blah*
but need plugins for other things.

## Page Inclusions

From PmWiki and IkiWiki. Be able to include one page in another page without breaking references and links etc.
This is tricky; one could get into a lot of recursion here.

## Sub-Pages

From PmWiki and other wikis. Use the page-inclusion mechanism to have separate pages for defining sidebars, headers and footers.

## Issues That Need Fixing

* Foil - breadcrumbs
* Foil - dynamic navbar
* Foil - dynamic title
